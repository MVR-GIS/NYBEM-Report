# Evaluation (I)

<span style="color: red;">McKay will complete by April 1.</span>

Ecological models such as NYBEM, typically rely on multiple variables, ecological processes, and in many cases present a variety of ecological outcomes. As such, models can quickly become complex system representations with many components, inputs, assumptions, and modules. Model evaluation is the process for ensuring that numerical tools are scientifically defensible and transparently developed. Evaluation is often referred to as verification or validation, but it in fact includes a family of methods ranging from peer review to model testing to error checking (Schmolke et al. 2010). In this more general sense, evaluation should include the following [@grant_ecological_2008]: (1) assessing the reasonableness of model structure, (2) assessing functional relationships and verifying code, (3) evaluating model behavior relative to expected patterns, (4) comparing outcomes to empirical data, if possible, and (5) analyzing uncertainty in predictions. The USACE has established an ecological model certification process to ensure that planning models are sound and functional. These generally consist of evaluating tools relative to the three following categories: system quality, technical quality, and usability (EC 1105-2-412).


## System Quality 

Ecological models must not only maintain an appropriate theoretical and technical basis, but also must be computationally accurate (McKay et al. 2021b). System quality refers to the computational integrity of a model (or modeling system). For instance, is the tool appropriately programmed, has it been verified or stress-tested, and do outcomes behave in expected ways? NYBEMâ€™s system quality was evaluated in a variety of ways, including the following:

- _Code checking:_ All code was error-checked during development by the primary programmer, and team members and interagency partners will be encouraged to inspect models as well. To date, quality assurance activities have included consistent variable naming, investigation of outputs from each individual line of code, and blocks of code (e.g., functions and loops).

- _Function Testing:_ Spatial zonation functions were tested with 100 randomized input sets of salinity, bed elevation, MHHW, and MLLW (Table 4). All outputs produced the anticipated outcomes based on manual inspection of the data. **ADD description of package testing with testthat.**


## Technical Quality

The NYBEM includes a number of parameters that have long been recognized as crucial to the health of coastal ecosystems. Model assessments were used or adapted from existing peer-reviewed models whenever possible (e.g., [@usace_evaluation_2009], plover ([@farmer_habitat_2000], [@seavey_effect_2011]), horseshoe crab ([@avissar_modeling_2006], [@nancy_jackson_armoring_2010], [@lathrope_mapping_2013], [@brady_habitat_1996]), amaranth [@sellars_habitat_2007], sea turtles [@dunkin_spatially_2016]).
Suitability curves and associated inputs, on the other hand, rely substantially on expert judgment and postulated coastal ecosystem functions. These methods have been found to have great utility and predictive value, and they are still widely utilized in ecological assessments (Hughes et al. 2010).

- Interagency workshops (Appendix A) for informal input and review.  

As a tool to examine the ecological impacts of infrastructure alternatives in a large, diversified ecosystem, the NYBEM is coupled to hydrodynamic model outcomes. This is a significant distinction between SLAMM and other long-term forecasting models. The inclusion of hydrodynamic model results is intended to quantify key physical processes within the ecosystem, with the model capable of influencing engineering design and assessing environmental impact. This model was used to create baseline conditions across the whole study area, and then construct habitat suitability curves to assess the potential impacts.

The technical quality of a model is assessed relative to its reliance on contemporary theory, consistency with design objectives, and degree of documentation and testing. NYBEM is being developed with a variety of ecological modeling methods for analyzing coastal ecosystems. The patch-scale quantity/quality framework has been applied extensively to assess outcomes ranging from the species to ecosystem levels of hierarchy (e.g., habitat evaluation procedures and the hydrogeomorphic methods, respectively). Furthermore, each sub-model is being developed with existing models and supported by peer reviewed methods developed elsewhere. System-scale connectivity models will do the same as well. To date, technical quality has been informally assessed through multiple development workshops.

__Future evaluations of technical quality will include:__

- Habitat zones will be compared to existing condition habitat maps through the National Wetlands Inventory and analogous state data sets (e.g., submerged aquatic vegetation and shellfish data sets).

- Informal review of model structure will continue with the interagency partners involved in development to date.

- Sensitivity analyses will be conducted to evaluate the relative importance of variables.


## Usability 

The usability of a model can influence the repeatable and transparent application of a tool. This type of evaluation typically examines the ease of use, availability of inputs, transparency, error potential, and education of the user. As such, defining the intended user(s) is a crucial component of assessing usability. The NYBEM was developed for application by the USACE technical team of the NJBB and HATS studies. The tool is not currently intended for broader application by local sponsors, other regional teams, or by other USACE Districts. As such, there is currently no graphical user interface for the model beyond the script itself. To date, the primary mechanism for maintaining usability is development in Rmarkdown which allows for transparent sharing of code within model documentation.
No graphic user interface (GUI) right now, but other methods were used to increase useability. Main talking points 

- Reproducible research methods.  

For a substantial and growing proportion of research experts developing novel statistical algorithms, R is the primary language. CRAN (Comprehensive R Archive Network) makes it simple to learn from others' work, share your own, and receive comments on possible changes. Packaging the NYBEM improves usability by providing a system for developing software that includes documentation and tests, resulting in higher software quality and development productivity.

Version control helps to maintain track of work and review the changes made to the NYBEM, whether they're to data, coding scripts, or notes. Version control is considerably smoother and easier to apply with version control software like Git. Furthermore, storing the NYBEM on the online platform Github creates an online backup, which is helpful to all contributors.

- Open source national and regional data sets and publishing of AdH outcomes.  

Utilizing previously-published high-value data sets in the NYBEM can increase transparency and accountability to the public and stakeholders. 

__Future evaluations of usability will include:__

* Open data sharing of all site-specific inputs and outputs for interrogation by internal and external partners.

* Training of project team members on the use of NYBEM.

* Archival and sharing of code with the ERDC ecological modeling team and the USACE Ecosystem Restoration Planning Center of Expertise.
